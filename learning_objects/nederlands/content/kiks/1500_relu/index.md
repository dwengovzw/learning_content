---
hruid: kiks_relu
version: 3
language: nl
title: "ReLU"
description: "ReLU"
keywords: ["AI"]
educational_goals: [
    {source: Source, id: id}, 
    {source: Source2, id: id2}
]
copyright: dwengo
licence: dwengo
content_type: text/markdown
available: true
target_ages: [16, 17, 18]
difficulty: 3
return_value: {
    callback_url: callback-url-example,
    callback_schema: {
        att: test,
        att2: test2
    }
}
content_location: example-location
estimated_time: 10
skos_concepts: [
    'http://ilearn.ilabt.imec.be/vocab/curr1/s-computers-en-systemen'
]
teacher_exclusive: false
---

# ReLU
De Heaviside-functie (drempelwaardefunctie), de identiteitsfunctie, de signfunctie, de sigmo√Øde en de ReLU-functie zijn *niet-lineaire functies*. Sommige van deze functies hebben een *meervoudig voorschrift*. <br>
Ze worden vaak aangewend als activatiefunctie in een neuraal netwerk. 

In het neurale netwerk van KIKS wordt ReLU gebruikt. Het is een niet-lineaire functie die het mogelijk maakt klassen die niet lineair scheidbaar zijn, toch te scheiden!<br> 
In deze notebook wordt uit de doeken gedaan hoe dat mogelijk is. Je zal inzien hoe krachtig deze techniek is. Heel indrukwekkend!<br>
Bovendien zal je beter begrijpen wat er zoal gebeurt in de *verschillende lagen* van een diep neuraal netwerk.

[![](embed/Knop.png "Knop")](https://kiks.ilabt.imec.be/jupyterhub/?id=1752 "Re LU")

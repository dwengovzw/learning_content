---
hruid: KIKS_relu-v1
version: 3
language: nl
title: "ReLU"
description: "ReLU"
keywords: ["AI"]
educational_goals: [
    {source: Source, id: id}, 
    {source: Source2, id: id2}
]
copyright: Copyright by Jerro
licence: Licenced by Jerro
content_type: text/markdown
available: true
target_ages: [16, 17, 18]
difficulty: 3
return_value: {
    callback_url: callback-url-example,
    callback_schema: {
        att: test,
        att2: test2
    }
}
content_location: example-location
estimated_time: 10
skos_concepts: [
    'http://ilearn.ilabt.imec.be/vocab/curr1/s-computers-en-systemen'
]
teacher_exclusive: true
---

# ReLU
De Heaviside-functie (drempelwaardefunctie), de identiteitsfunctie, de signfunctie, de sigmo√Øde en de ReLU-functie zijn niet-lineaire functies. Ze worden
vaak aangewend als activatiefunctie in een neuraal netwerk. In het neurale netwerk van KIKS wordt ReLU gebruikt. Het is een niet-lineaire functie die het mogelijk maakt klassen die niet lineair scheidbaar zijn, toch te scheiden. In deze notebook wordt dit uit de doeken gedaan. 

[![](embed/Knop.png "Knop")](https://kiks.ilabt.imec.be/jupyterhub/?id=1752 "Re LU")

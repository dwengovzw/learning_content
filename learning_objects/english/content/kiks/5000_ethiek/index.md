---
available: true
content_location: example-location
content_type: text/markdown
copyright: dwengo
description: Ethics
difficulty: 3
educational_goals:
- id: id
  source: Source
- id: id2
  source: Source2
estimated_time: 10
hruid: kiks_ethiek
keywords:
- ''
language: en
licence: dwengo
return_value:
  callback_schema:
    att: test
    att2: test2
  callback_url: callback-url-example
skos_concepts:
- http://ilearn.ilabt.imec.be/vocab/curr1/s-computers-en-systemen
target_ages:
- 16
- 17
- 18
teacher_exclusive: true
title: Ethics
version: 3
---
# Ethics
When AI technology is used correctly, it can contribute to a *better world*. Think of exoskeletons making people with disabilities more mobile,
seniors able to live at home longer, more efficient food production, more economical electric appliances, safer work by using robots in the
demining service.

*But will AI always be used for a better society?* In the development of AI systems, one can be driven by various motives.
Will the development of a new product always center around human needs, or sometimes also around the AI system itself or profit?

By employing robots in the healthcare sector, nurses get more time for the patients. However, if one chooses to therefore hire less staff, then the human aspect is lost. Using technology in medicine can be a positive thing, but how far does one want to go in that? What kind of
cyborg is permissible? Is a virtual therapist like Woebot desirable? What tasks will care robots have? How does one ensure that exoskeletons are accessible to everyone and not just for people with enough money (Gabriels, 2019)?

Under the guise of fighting crime, AI systems can be used to surveil people everywhere. Then the privacy of individuals is compromised. A route planner also makes suggestions for restaurants in the vicinity of the route. How is it determined which restaurants are shown and
what motives are involved in that? Thanks to AI, some platforms can deliver interesting job advertisements to oneâ€™s inbox. But does one actually see all the job adverts one is interested in?

Until 2018, Amazon used a highly acclaimed but now discontinued AI system to evaluate applicants. The system did not select women
for technology positions. It was trained with historical data: applicants from the past 10 years, predominantly men, since they still dominate the
technology world. Google's voice recognition works better on male voices because the database of voice recordings used to train the system contained far
more male voices. During a beauty contest in 2016, judged by an AI system, participants with darker skin were disadvantaged because the algorithm was trained with photos of predominantly white people. Contrary to popular belief, an AI system does not make 100% 'objective' decisions.

An AI system is 'biased', usually due to the data with which the system was trained. This **bias** must be controlled as much as possible and reduced to the
minimum. AI system developers must be aware of potential undesirable effects from the outset. The moral responsibility of an AI system that is introduced into society lies not with the AI system, but with the human.
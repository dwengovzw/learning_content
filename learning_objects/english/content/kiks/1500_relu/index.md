---
available: true
content_location: example-location
content_type: text/markdown
copyright: dwengo
description: ReLU
difficulty: 3
educational_goals:
- id: id
  source: Source
- id: id2
  source: Source2
estimated_time: 10
hruid: kiks_relu
keywords:
- AI
language: en
licence: dwengo
return_value:
  callback_schema:
    att: test
    att2: test2
  callback_url: callback-url-example
skos_concepts:
- http://ilearn.ilabt.imec.be/vocab/curr1/s-computers-en-systemen
target_ages:
- 16
- 17
- 18
teacher_exclusive: false
title: ReLU
version: 3
---
# ReLU
The Heaviside function (threshold function), the identity function, the sign function, the sigmoid, and the ReLU function are *non-linear functions*. Some of these functions have a *multiple prescription*. <br>
They are often used as activation functions in a neural network.

In the neural network of KIKS, ReLU is used. It is a non-linear function that makes it possible to separate classes that are not linearly separable!<br>
This notebook explains how this is possible. You will see how powerful this technique is. Very impressive!<br>
Moreover, you will understand better what happens in the *different layers* of a deep neural network.

[![](embed/Knop.png "Button")](https://kiks.ilabt.imec.be/jupyterhub/?id=1752 "Re LU")
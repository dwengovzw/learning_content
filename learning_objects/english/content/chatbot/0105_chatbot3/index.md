---
available: true
content_type: text/markdown
copyright: dwengo
description: Large language models
difficulty: 3
educational_goals:
- id: id
  source: Source
- id: id2
  source: Source2
estimated_time: 10
hruid: cb_chatbot3
keywords:
- voorbeeld
- voorbeeld2
language: en
licence: dwengo
return_value:
  callback_schema:
    att: test
    att2: test2
  callback_url: callback-url-example
skos_concepts:
- http://ilearn.ilabt.imec.be/vocab/curr1/s-digitale-media-en-toepassingen
- http://ilearn.ilabt.imec.be/vocab/curr1/s-computers-en-systemen
- http://ilearn.ilabt.imec.be/vocab/curr1/s-stem-onderzoek
- http://ilearn.ilabt.imec.be/vocab/curr1/s-wiskunde-modelleren-en-heuristiek
target_ages:
- 14
- 15
- 16
- 17
- 18
teacher_exclusive: true
title: Large language models
version: 3
---
# Large language models 
## GPT, BERT and GPT-2
In 2018 there was a breakthrough in recognizing and generating language when the research institute OpenAI came up with a deep learning system
to generate text, a large language model (LLM): Generative Pre-trained Transformer, in short GPT. In the same year Google also launched its own deep learning language system, BERT, which stands for Bidirectional Encoder Representations from Transformers.<br>

These systems can predict the next word in a text based on the preceding words in the text. They are computational language models that were trained with techniques from machine learning, i.e., they learned from examples. <br>
In 2019 OpenAI already introduced a successor, GPT-2. For GPT-2 a gigantic dataset of examples was generated by collecting web pages with a bot. This bot surfed the internet by following hyperlinks on Reddit, namely hyperlinks to web pages that many Reddit users had marked as interesting, educational, or funny.<br> 

> OpenAI was founded in 2015 by Sam ALtman and Elon Musk.

In addition to completing a text, GPT-2 is able to generate text within a conversation. GPT-2 can also translate text, answer questions about the content of a text, provide a caption for an image, or summarize a text. GPT-2 can therefore do quite a lot. <br>
The downside is that the training of GPT-2, in which so much data had to be processed, required a great deal of energy.

> GPT, BERT and GPT-2 are 'open source' systems. That means the code is public and that anyone is free to use and modify the code, for example to develop a new application. 

Example:
-  The Flemish company ML6 adapted GPT-2 to be able to generate texts in Dutch as well (Dehaene, 2020). [Their system](https://gpt2.ml6.eu/nl) wrote an article for the magazine Data News, ‘Will robots replace the job of journalist?’ (VLAIO, 2020). You can read this article in the manual of the 'Chatbot' project on p. 43.

*Thanks to the internet and crawlers, it is possible to build immense databases on which these systems are trained. Because these systems are trained with data originating from the World Wide Web, they are extra sensitive to bias.*
 

## GPT-3 and ChatGPT
In 2020 GPT-3 appeared, a deep neural network that was trained on even more data than GPT-2, including the entire English-language Wikipedia. According to OpenAI, GPT-3 is "usable for almost any English language task”.<br>
The training data of GPT-3 includes, among other things, English-language books, the English-language Wikipedia, data collected over years via the internet, and millions of web pages.

> GPT-3 is not an 'open source' system. In 2020 Microsoft obtained the licenses for GPT-3. But via the web one can find more than 300 applications based on GPT-3 (OpenAI, 2021).

In the meantime there has been a lot of experimentation with the possibilities of GPT-3, such as writing poetry, developing chatbots and websites, and programming in Python:
-  In 2020 GPT-3 already wrote an opinion piece for The Guardian: ‘A robot wrote this entire article. Are you scared yet, human?’. (See the manual of the 'Chatbot' project p. 46.)
-  A [tool](https://www.usetopic.com/blog-idea-generator) has been developed to generate a blog idea. This tool enters the input keywords into Google and then GPT-3 generates a blog idea based on the content of the highest-ranked search results. (See the manual of the 'Chatbot' project.)
-  GPT-3 is sometimes [used in combination with image recognition](https://javifuentes94-clip-playground-app-haepeo.streamlit.app/). Language and image thus lead, for example, to a classification system. (See the manual of the 'Chatbot' project p. 48.)
-  Improbotics first used GTP-2 to generate the conversations of the social robot Alex who acts in improvisational theater, see [the Improbotics webpage](http://www.erlnmyr.be/voorstellingen/improbotics/), but has recently switched to GPT-3.
-  In the section ‘ChatGPT and co’ of this learning path you can read how ChatGPT originated from GPT-3. 

## How these large language models work

AI systems that process language are not automatically able to work with natural language. They must be provided the texts in a numerical form. The large language models make use of word embeddings. Those embeddings are created with AI techniques, such as Google’s word2vec; fortunately they do not have to be created manually. <br>
The words in a text are therefore first converted into vectors. A vector can be seen as a finite sequence of numbers. Word embedding means that one tries to capture as much information about the words as possible in these numbers: to determine the sequence of numbers that represents a particular word, account is taken of the meaning of the word, which other words co-occur with the word in many sentences, the position of the word in a sentence, the context of a sentence in which the word occurs. 

![Word embedding](embed/wordtovec1.png "Word embedding word2vec")
<center>Relationships between words with word2vec. Embeddings can produce remarkable analogies (Google, 2020).</center> 

<br>
<em>Embeddings</em> thus convert text into vectors. In word2vec, <b>semantically related texts are placed close to each other in the vector space</b>. For example, because the words ‘cat’ and ‘dog’ often occur together with the word ‘veterinarian’ in texts, one can find the corresponding three vectors close together in the vector space. Similar relationships between words can also often be found back in the vector space (see Figure).<br><br>

> The NLP technique word embedding is also used by search engines. With the help of word embedding, a web search query yields better results. The search engine will, besides the actual search query, also look for words and terms that are related to it. 

> Thanks to this technique, automatic translations have also made great strides. These used to not work well because they were programmed rule-based and could not recognize context. To translate a text, a modern translation program, among other things, determines which word is most likely to follow a given sequence of words. Thus it takes into account that after ‘I buy a’ usually a noun or an adjective with a noun follows. 

<div class="alert alert-box alert-info">
    Murray Shanahan says that we should be keenly aware of what a large language model does. "Suppose we give the following prompt to an LLM: “The first person to walk on the Moon was”,  and suppose it answers with “Neil Armstrong”. What are we actually asking here? It is important to realize that we are not actually asking who the first person to walk on the Moon was. The question we are really asking the model is the following: Given the statistical distribution of the words in the extensive public corpus of (English) text, which words are most likely to follow the sequence “The first person to walk on the Moon was ”? A good answer to this question is “Neil Armstrong” (Source: Talking About Large Language Models, Murray Shanahan, 2022).
</div>

## Example

![Math problem](embed/chatgptrekent.png "Math problem is interpreted by ChatGPT")

![Same math problem later](embed/rekensomuuranalogie.png "Better performance by ChatGPT at a later date")